{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oxford Pets Image Segmentation\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Dataset Preparation](#dataset-prep)\n",
    "3. [Model](#model-def)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a id=\"setup\"></a>\n",
    "This section handles the imports and configuration of paths and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps\n",
    "from keras.utils import load_img\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from data_set import load_file_paths, get_datasets\n",
    "\n",
    "# Ensure dataset folder exists\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# Global parameters\n",
    "input_dir = \"dataset/images/\"\n",
    "target_dir = \"dataset/annotations/trimaps/\"\n",
    "img_size = (160, 160)\n",
    "batch_size = 32\n",
    "num_classes = 3\n",
    "val_samples = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Extract Dataset <a id=\"dataset-prep\"></a>\n",
    "In this section, we load the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the Oxford Pets dataset\n",
    "!wget -P dataset https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "!wget -P dataset https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "\n",
    "!tar -xf dataset/images.tar.gz -C dataset\n",
    "!tar -xf dataset/annotations.tar.gz -C dataset\n",
    "\n",
    "print(\"Dataset downloaded and extracted successfully.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model <a id=\"model\"></a>\n",
    "Train and visualize some predictions from the validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data_set import get_datasets, preprocess_image_mask\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import io as tf_io\n",
    "import data_set\n",
    "import importlib\n",
    "importlib.reload(data_set)\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "input_dir = \"dataset/images/\"\n",
    "target_dir = \"dataset/annotations/trimaps/\"\n",
    "batch_size = 16\n",
    "img_size = (160, 160)\n",
    "\n",
    "# Debug datasets\n",
    "train_dataset, val_dataset = get_datasets(input_dir, target_dir, batch_size, img_size=img_size)\n",
    "\n",
    "# Data augmentation\n",
    "def augment_data(image, mask):\n",
    "    # Random horizontal flip\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "\n",
    "    # Random brightness adjustment\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.adjust_brightness(image, delta=0.2)\n",
    "\n",
    "    # Random contrast adjustment\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.adjust_contrast(image, contrast_factor=0.8)\n",
    "\n",
    "    # Random rotation\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.rot90(image)\n",
    "        mask = tf.image.rot90(mask)\n",
    "\n",
    "    # Random gussian noise\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "      noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.05, dtype=tf.float32)\n",
    "      image = tf.clip_by_value(image + noise, 0.0, 1.0)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x, y: augment_data(x, y), \n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Define U-Net-like model\n",
    "def get_unet_model(img_size, num_classes):\n",
    "    inputs = tf.keras.Input(shape=img_size + (3,))\n",
    "    \n",
    "    # Downsampling\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    previous_block_activation = x\n",
    "    \n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if filters == 256:  # Only apply dropout to the deepest layer\n",
    "          x = layers.Dropout(0.2)(x)\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x\n",
    "\n",
    "    # Upsampling\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x\n",
    "\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Load the model\n",
    "model = get_unet_model(img_size, num_classes)\n",
    "\n",
    "# Class weights \n",
    "class_weights = {0: 1.0, 1: 0.5, 2: 2.5}  \n",
    "\n",
    "def weighted_sparse_categorical_crossentropy(class_weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = K.cast(tf.squeeze(y_true, axis=-1), 'int32')\n",
    "        weights = tf.gather(tf.constant(list(class_weights.values())), y_true)  \n",
    "        scce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred) \n",
    "        return K.mean(weights * scce) \n",
    "    return loss\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    #loss= \"sparse_categorical_crossentropy\",\n",
    "    loss=weighted_sparse_categorical_crossentropy(class_weights),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.95\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=23,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[lr_callback]\n",
    ")\n",
    "\n",
    "for img, mask in val_dataset.take(1):  \n",
    "    pred_mask = model.predict(img)  \n",
    "    pred_mask_class = tf.argmax(pred_mask, axis=-1).numpy() \n",
    "\n",
    "    # Visualize the first image and its predicted mask\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Input image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(tf.squeeze(img[0]).numpy())  \n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Ground truth mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(tf.squeeze(mask[0]).numpy(), cmap=\"gray\")  \n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Predicted mask\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred_mask_class[0], cmap=\"gray\")  \n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#history = model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Plot accuracy and loss\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
